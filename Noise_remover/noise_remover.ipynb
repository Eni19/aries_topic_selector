{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58582b4c",
   "metadata": {},
   "source": [
    "Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pt_lemmatizer import Lemmatizer\n",
    "import spacy\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 42\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d478d31",
   "metadata": {},
   "source": [
    "Fun√ß√£o de Detetc√ß√£o de Linguagens utilizando langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fbb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672030fb",
   "metadata": {},
   "source": [
    "Fun√ß√µes de Tratamento de Textos para Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8d6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_18500\\1980745371.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n"
     ]
    }
   ],
   "source": [
    "punctuation =\"‚Äò!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@‚Äô\"\n",
    "\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380c937",
   "metadata": {},
   "source": [
    "Usamos os Spacy para dividir em Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8175973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texto):\n",
    "    \"\"\"Tokeniza, remove stopwords e lematiza um texto em portugu√™s\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    return [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and token.is_alpha and len(token.text) > 2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee707e",
   "metadata": {},
   "source": [
    "Exemplo de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06651205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rato', 'roer', 'roupa', 'rei', 'roma']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"Os ratos roeram a roupa do rei de Roma.\"))\n",
    "# Sa√≠da exemplo: ['rato', 'roer', 'roupa', 'rei', 'Roma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fa210",
   "metadata": {},
   "source": [
    "Fun√ß√µes de PreProcessamento de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1005a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_18500\\316698526.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_18500\\316698526.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def basic_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e5dd",
   "metadata": {},
   "source": [
    "Aplica√ß√£o no Dataframe de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775e52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df['clean_tweet'] = df.Content.apply(basic_clean)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.Content.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4795f84",
   "metadata": {},
   "source": [
    "Execu√ß√£o do C√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571c7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'URL', 'Content', 'Likes', 'Retweets', 'Replies', 'Quotes',\n",
      "       'Views', 'Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\dados\\dadosTwitter\\CSV\\tweets amoxilina - P√°gina1.csv'\n",
    "tweets_df = pd.read_csv(caminho_arquivo)\n",
    "tweets_df.dropna(axis='columns', inplace=True)\n",
    "print(tweets_df.columns)\n",
    "\n",
    "#Escolhendo as colunas relevantes\n",
    "tweets_df = tweets_df[['Tweet ID', 'Content', 'Date']]\n",
    "\n",
    "\n",
    "#S√≥ por precau√ß√£o, removendo duplicatas\n",
    "tweets_df.drop_duplicates(inplace=True, subset=\"Tweet ID\")\n",
    "tweets_df.drop_duplicates(inplace=True, subset=\"Content\")\n",
    "\n",
    "#print(tweets_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b24a33",
   "metadata": {},
   "source": [
    "Agora, separamos os tweets por Idiomas utilizando a Biblioteca LangDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3382597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned and tokenized : 202\n"
     ]
    }
   ],
   "source": [
    "tweets_df = clean_tweets(tweets_df)\n",
    "tweets_df[\"language\"] = tweets_df[\"Content\"].apply(detect_language)\n",
    "\n",
    "#Criando dataframes separados por idioma\n",
    "\n",
    "df_espanhol = tweets_df[tweets_df[\"language\"] == \"es\"]\n",
    "df_ingles = tweets_df[tweets_df[\"language\"] == \"en\"]\n",
    "df_portugues = tweets_df[~tweets_df[\"language\"].isin([\"es\", \"en\"])]\n",
    "\n",
    "df_espanhol.to_excel(\"tweets_processados_espanhol.xlsx\", index=False)\n",
    "df_ingles.to_excel(\"tweets_processados_ingles.xlsx\", index=False)\n",
    "df_portugues.to_excel(\"tweets_processados_portugues.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d9dbb",
   "metadata": {},
   "source": [
    "Agora, aplicas o Tokenizer no datframe em Portugues, que √© o nosso foco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d90872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned and tokenized : 187\n",
      "Arquivo 'tweets_processados.xlsx' salvo com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_18500\\4228821110.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df.Content.apply(preprocess_tweet)\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_18500\\3098012852.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_portugues[\"word_count\"] = df_portugues[\"tokens\"].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "df_portugues = tokenize_tweets(df_portugues)\n",
    "\n",
    "#Removendo tweets com menos de 6 palavras\n",
    "df_portugues[\"word_count\"] = df_portugues[\"tokens\"].apply(lambda x: len(x.split()))\n",
    "df_portugues = df_portugues[df_portugues[\"word_count\"] >= 6]\n",
    "\n",
    "\n",
    "df_portugues.to_excel(\"tweets_processados.xlsx\", index=False)\n",
    "print(\"Arquivo 'tweets_processados.xlsx' salvo com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
