{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58582b4c",
   "metadata": {},
   "source": [
    "Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pt_lemmatizer import Lemmatizer\n",
    "import spacy\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 42\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d478d31",
   "metadata": {},
   "source": [
    "Fun√ß√£o de Detetc√ß√£o de Linguagens utilizando langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fbb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672030fb",
   "metadata": {},
   "source": [
    "Fun√ß√µes de Tratamento de Textos para Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8d6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_14952\\3105653607.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n"
     ]
    }
   ],
   "source": [
    "punctuation =\"‚Äò!‚Äù$%&\\‚Äô()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@‚Äô\"\n",
    "\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def remove_risadas(tweet):\n",
    "    tweet = re.sub(r'\\b[kK]{2,}\\b', '', tweet)  # kkk, kkkkk...\n",
    "    tweet = re.sub(r'\\b(?:[hH][aA]){2,}\\b', '', tweet)  # haha, hahahaha...\n",
    "    tweet = re.sub(r'\\b(?:[jJ][aA]){2,}\\b', '', tweet)  # jaja, jajajaja... (m√≠nimo 2x)\n",
    "    tweet = re.sub(r'\\b(?:[rR][sS]){2,}\\b', '', tweet)  # rsrs, rsrsrsrs...\n",
    "    tweet = re.sub(r'\\b(?:[kK][sS]){2,}\\b', '', tweet)  # ksks, ksksksks...\n",
    "    return tweet\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    \"\"\"Remove emojis e caracteres especiais n√£o ASCII comuns em emotes\"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons üòÄ-üôè\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos e pictogramas üåß-üóø\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transporte e mapas üöÄ-üöß\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # bandeiras üáßüá∑-üá∫üá∏\n",
    "        \"\\U00002700-\\U000027BF\"  # s√≠mbolos diversos ‚úÇ-‚ûø\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # emojis suplementares ü§ñ-üß†\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # objetos adicionais ü©∞-ü™Ö\n",
    "        \"\\U00002500-\\U00002BEF\"  # caixas, setas, ideogramas\n",
    "        \"\\U0001F018-\\U0001F270\"  # v√°rios outros\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380c937",
   "metadata": {},
   "source": [
    "Usamos os Spacy para dividir em Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8175973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texto):\n",
    "    \"\"\"Tokeniza, remove stopwords e lematiza um texto em portugu√™s\"\"\"\n",
    "    \n",
    "    custom_stopwords = {\"kkk\", \"bom dia\", \"pra\", \"dia\", \"de\"}\n",
    "    for word in custom_stopwords:\n",
    "        nlp.vocab[word].is_stop = True\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    return [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and token.is_alpha and len(token.text) > 2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee707e",
   "metadata": {},
   "source": [
    "Exemplo de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06651205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rato', 'roer', 'roupa', 'rei', 'roma']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"Os ratos roeram a roupa do rei de Roma.\"))\n",
    "# Sa√≠da exemplo: ['rato', 'roer', 'roupa', 'rei', 'Roma']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fa210",
   "metadata": {},
   "source": [
    "Fun√ß√µes de PreProcessamento de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1005a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_14952\\503958352.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_14952\\503958352.py:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = remove_risadas(tweet)\n",
    "    tweet = remove_emojis(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def basic_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = remove_risadas(tweet)\n",
    "    tweet = remove_emojis(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e5dd",
   "metadata": {},
   "source": [
    "Aplica√ß√£o no Dataframe de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775e52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df['clean_tweet'] = df.Content.apply(basic_clean)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.Content.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize_news(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.Corpo.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "def tokenize_gov(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.Conteudo_Materia.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been tokenized : {}'.format(num_tweets))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df649edd",
   "metadata": {},
   "source": [
    "# Esse trecho se dedica a Tratar os dados de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4795f84",
   "metadata": {},
   "source": [
    "### Carregar a planilha e selecionar colunas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "571c7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'URL', 'Content', 'Likes', 'Retweets', 'Replies', 'Quotes',\n",
      "       'Views', 'Date'],\n",
      "      dtype='object')\n",
      "                 Tweet ID                                            Content  \\\n",
      "0     1932527926723182644            I hate Tuberculose with all of my heart   \n",
      "1     1932521806503858629  Je suis pas pr√©par√©e √† voir la Tuberculose en ...   \n",
      "2     1932513994868187138  vei esse m√≥dulo de ecla n faz sentido nenhum u...   \n",
      "3     1932507835130368147  @weedtonina @livbadbiatch @ogmtbmr Bgl √© s√©rio...   \n",
      "4     1932501301222650086  Je vais trouver la personne qui m‚Äôa mis cas co...   \n",
      "...                   ...                                                ...   \n",
      "7882  1889391439034876258  @MarionMarechal @ecrgroup @LaurenceTrochu @G_P...   \n",
      "7883  1889391168334516509  Tuberculose-behandelprogramma's stopzetten is ...   \n",
      "7884  1889391125984686508  @MarionMarechal je me suis mordre par le sopor...   \n",
      "7885  1889388371497152671  @ansecivoc Tja, een beetje van alles plus tube...   \n",
      "7886  1889387240494866817                                       eu anteontem   \n",
      "\n",
      "                               Date  \n",
      "0         June 10, 2025 at 07:58 PM  \n",
      "1         June 10, 2025 at 07:34 PM  \n",
      "2         June 10, 2025 at 07:03 PM  \n",
      "3         June 10, 2025 at 06:39 PM  \n",
      "4         June 10, 2025 at 06:13 PM  \n",
      "...                             ...  \n",
      "7882  February 11, 2025 at 07:09 PM  \n",
      "7883  February 11, 2025 at 07:08 PM  \n",
      "7884  February 11, 2025 at 07:08 PM  \n",
      "7885  February 11, 2025 at 06:57 PM  \n",
      "7886  February 11, 2025 at 06:53 PM  \n",
      "\n",
      "[7838 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\dados\\dadosTwitter\\CSV\\tweets tuberculose.csv'\n",
    "tweets_df = pd.read_csv(caminho_arquivo)\n",
    "#tweets_df.dropna(axis='columns', inplace=True)\n",
    "print(tweets_df.columns)\n",
    "\n",
    "#Escolhendo as colunas relevantes\n",
    "tweets_df = tweets_df[['Tweet ID', 'Content', 'Date']]\n",
    "\n",
    "\n",
    "#S√≥ por precau√ß√£o, removendo duplicatas\n",
    "tweets_df.drop_duplicates(inplace=True, subset=\"Tweet ID\")\n",
    "tweets_df.drop_duplicates(inplace=True, subset=\"Content\")\n",
    "\n",
    "print(tweets_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b24a33",
   "metadata": {},
   "source": [
    "Agora, separamos os tweets por Idiomas utilizando a Biblioteca LangDetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3382597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned : 7838\n"
     ]
    }
   ],
   "source": [
    "tweets_df = tweets_df.dropna(subset=[\"Content\"])\n",
    "tweets_df = clean_tweets(tweets_df)\n",
    "tweets_df[\"language\"] = tweets_df[\"Content\"].apply(detect_language)\n",
    "\n",
    "#Criando dataframes separados por idioma\n",
    "\n",
    "df_espanhol = tweets_df[tweets_df[\"language\"] == \"es\"]\n",
    "df_ingles = tweets_df[tweets_df[\"language\"] == \"en\"]\n",
    "df_ingles = tweets_df[tweets_df[\"language\"] == \"fr\"]\n",
    "df_portugues = tweets_df[~tweets_df[\"language\"].isin([\"es\", \"en\", \"fr\"])]\n",
    "\n",
    "df_espanhol.to_excel(\"tweets_processados_espanhol.xlsx\", index=False)\n",
    "df_ingles.to_excel(\"tweets_processados_ingles.xlsx\", index=False)\n",
    "df_portugues.to_excel(\"tweets_processados_portugues.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d9dbb",
   "metadata": {},
   "source": [
    "Agora, aplicas o Tokenizer no datframe em Portugues, que √© o nosso foco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d90872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been tokenized : 5074\n",
      "Arquivo 'tweets_processados.csv' salvo com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_10228\\38493525.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df.Content.apply(preprocess_tweet)\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_10228\\4190352179.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_portugues[\"word_count\"] = df_portugues[\"tokens\"].apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_portugues = tokenize_tweets(df_portugues)\n",
    "\n",
    "#Removendo tweets com menos de 6 palavras\n",
    "df_portugues[\"word_count\"] = df_portugues[\"tokens\"].apply(lambda x: len(x.split()))\n",
    "df_portugues = df_portugues[df_portugues[\"word_count\"] >= 6]\n",
    "df_portugues = df_portugues.drop(columns=[\"word_count\"])\n",
    "df_portugues.drop_duplicates(inplace=True, subset=\"tokens\")\n",
    "df_portugues.to_csv(\"tweets_processados.csv\", index=False)\n",
    "print(\"Arquivo 'tweets_processados.csv' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb997109",
   "metadata": {},
   "source": [
    "Se quiser salvar como Xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a409e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'tweets_processados.xlsx' salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "df_portugues.to_excel(\"tweets_processados.xlsx\", index=False)\n",
    "print(\"Arquivo 'tweets_processados.xlsx' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4805883",
   "metadata": {},
   "source": [
    "# Esse trecho se dedica a tratar os trechos para Not√≠cias do G1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23870c9b",
   "metadata": {},
   "source": [
    "### Carregar planilha e selecionar colunas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e850e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Titulo', 'Corpo', 'Link', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n",
      "       'Unnamed: 6'],\n",
      "      dtype='object')\n",
      "['Titulo', 'Corpo', 'Link', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6']\n",
      "                                                 Titulo  \\\n",
      "0     Anvisa suspende amoxicilina e rifamicina da fa...   \n",
      "1     Lotes de amoxicilina s√£o proibidos de serem co...   \n",
      "2                                 T√≠tulo n√£o encontrado   \n",
      "7     Anvisa suspende por 90 dias venda de antibi√≥ti...   \n",
      "8     Rem√©dios e vacinas s√£o encontrados no meio de ...   \n",
      "...                                                 ...   \n",
      "4354  Tuberculose no Par√° foi tema de reuni√£o com o ...   \n",
      "4355  Hospital define local de exames ap√≥s casos de ...   \n",
      "4357  N√∫mero de pessoas com tuberculose recua pela 1...   \n",
      "4358  Casos de tuberculose no mundo caem pela primei...   \n",
      "4361  N√∫mero de crian√ßas com tuberculose em Campinas...   \n",
      "\n",
      "                                                  Corpo  \n",
      "0     04/02/2015 20h31- Atualizado em04/02/2015 20h3...  \n",
      "1     11/05/2010 13h34- Atualizado em11/05/2010 13h3...  \n",
      "2     Por Kleber Tomaz, G1 SP‚Äî S√£o Paulo 05/11/2019 ...  \n",
      "7     20/09/2016 17h31- Atualizado em20/09/2016 19h0...  \n",
      "8     21/05/2014 21h40- Atualizado em21/05/2014 21h4...  \n",
      "...                                                 ...  \n",
      "4354  23/08/2012 09h19- Atualizado em23/08/2012 09h1...  \n",
      "4355  21/09/2012 09h01- Atualizado em21/09/2012 09h0...  \n",
      "4357  11/10/2011 11h22- Atualizado em11/10/2011 12h1...  \n",
      "4358  11/10/2011 12h18- Atualizado em11/10/2011 12h1...  \n",
      "4361  08/11/2012 17h51- Atualizado em08/11/2012 17h5...  \n",
      "\n",
      "[2003 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\dados\\dadosG1\\unificado\\unificado.csv'\n",
    "news_df = pd.read_csv(caminho_arquivo, delimiter=',')\n",
    "\n",
    "print(news_df.columns)\n",
    "print(list(news_df.columns))\n",
    "\n",
    "\n",
    "#Escolhendo as colunas relevantes\n",
    "news_df = news_df[['Titulo', 'Corpo']]\n",
    "\n",
    "\n",
    "#S√≥ por precau√ß√£o, removendo duplicatas\n",
    "news_df.drop_duplicates(inplace=True, subset=\"Titulo\")\n",
    "news_df.drop_duplicates(inplace=True, subset=\"Corpo\")\n",
    "\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248320c",
   "metadata": {},
   "source": [
    "Agora limpamos o dataFrame e usamos o tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699d2151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been tokenized : 2002\n",
      "Arquivo 'noticias_g1_processados.csv' salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Suponha que seu DataFrame se chame `df` e a coluna com o texto seja \"Corpo\"\n",
    "news_df = news_df.dropna(subset=[\"Corpo\"])\n",
    "news_df['Corpo'] = news_df['Corpo'].str.replace(\n",
    "    r'^\\d{2}/\\d{2}/\\d{4}\\s\\d{2}h\\d{2}-\\s*Atualizado em\\d{2}/\\d{2}/\\d{4}\\s\\d{2}h\\d{2}\\s*,?\\s*',\n",
    "    '',\n",
    "    regex=True\n",
    ")\n",
    "news_df = tokenize_news(news_df)\n",
    "\n",
    "#Removendo tweets com menos de 6 palavras\n",
    "news_df[\"word_count\"] = news_df[\"tokens\"].apply(lambda x: len(x.split()))\n",
    "news_df = news_df[news_df[\"word_count\"] >= 6]\n",
    "news_df = news_df.drop(columns=[\"word_count\"])\n",
    "news_df.drop_duplicates(inplace=True, subset=\"tokens\")\n",
    "news_df.to_csv(\"news_processados.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Arquivo 'noticias_g1_processados.csv' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e577d637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'news_processados.xlsx' salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "news_df.to_excel(\"news_processados.xlsx\", index=False)\n",
    "print(\"Arquivo 'news_processados.xlsx' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edbf502f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Corpo</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anvisa suspende amoxicilina e rifamicina da fa...</td>\n",
       "      <td>Do G1, em S√£o Paulo A Anvisa anunciou nesta qu...</td>\n",
       "      <td>paulo anvisa anunciar feira suspens√£o antibi√≥t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lotes de amoxicilina s√£o proibidos de serem co...</td>\n",
       "      <td>Do RJTV Conte√∫do n√£o dispon√≠vel. Infelizmente ...</td>\n",
       "      <td>rjtv conte√∫do dispon√≠vel infelizmente v√≠deo di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T√≠tulo n√£o encontrado</td>\n",
       "      <td>Por Kleber Tomaz, G1 SP‚Äî S√£o Paulo 05/11/2019 ...</td>\n",
       "      <td>kleber tomaz paulo hatualizadoh√° ano antibi√≥ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anvisa suspende por 90 dias venda de antibi√≥ti...</td>\n",
       "      <td>Do G1, em S√£o Paulo A Ag√™ncia Nacional de Vigi...</td>\n",
       "      <td>paulo ag√™ncia nacional vigil√¢ncia sanit√°ria an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rem√©dios e vacinas s√£o encontrados no meio de ...</td>\n",
       "      <td>Do G1 Ribeir√£o e Franca  Jovens entre 6 e 17 a...</td>\n",
       "      <td>ribeir√£o franca jovem ano aprender instrumento...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>Tuberculose no Par√° foi tema de reuni√£o com o ...</td>\n",
       "      <td>Do G1 PA A situa√ß√£o da tuberculose no Par√° foi...</td>\n",
       "      <td>situa√ß√£o tuberculose par tema reuni√£o secretar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>Hospital define local de exames ap√≥s casos de ...</td>\n",
       "      <td>Do G1 Campinas e Regi√£o  O Hospital e Maternid...</td>\n",
       "      <td>campina regi√£o hospital maternidade madre theo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>N√∫mero de pessoas com tuberculose recua pela 1...</td>\n",
       "      <td>Do G1, com informa√ß√µes da Reuters O n√∫mero de ...</td>\n",
       "      <td>informa√ß√£o reuter pessoa doente conta tubercul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>Casos de tuberculose no mundo caem pela primei...</td>\n",
       "      <td>Reuters (Reuters) - O n√∫mero de pessoas que ad...</td>\n",
       "      <td>reuter reuters pessoa adoecer tuberculose cair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>N√∫mero de crian√ßas com tuberculose em Campinas...</td>\n",
       "      <td>Do G1 Campinas e Regi√£o O n√∫mero de beb√™s infe...</td>\n",
       "      <td>campina regi√£o beb√™ infectar tuberculose hospi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Titulo  \\\n",
       "0     Anvisa suspende amoxicilina e rifamicina da fa...   \n",
       "1     Lotes de amoxicilina s√£o proibidos de serem co...   \n",
       "2                                 T√≠tulo n√£o encontrado   \n",
       "7     Anvisa suspende por 90 dias venda de antibi√≥ti...   \n",
       "8     Rem√©dios e vacinas s√£o encontrados no meio de ...   \n",
       "...                                                 ...   \n",
       "4354  Tuberculose no Par√° foi tema de reuni√£o com o ...   \n",
       "4355  Hospital define local de exames ap√≥s casos de ...   \n",
       "4357  N√∫mero de pessoas com tuberculose recua pela 1...   \n",
       "4358  Casos de tuberculose no mundo caem pela primei...   \n",
       "4361  N√∫mero de crian√ßas com tuberculose em Campinas...   \n",
       "\n",
       "                                                  Corpo  \\\n",
       "0     Do G1, em S√£o Paulo A Anvisa anunciou nesta qu...   \n",
       "1     Do RJTV Conte√∫do n√£o dispon√≠vel. Infelizmente ...   \n",
       "2     Por Kleber Tomaz, G1 SP‚Äî S√£o Paulo 05/11/2019 ...   \n",
       "7     Do G1, em S√£o Paulo A Ag√™ncia Nacional de Vigi...   \n",
       "8     Do G1 Ribeir√£o e Franca  Jovens entre 6 e 17 a...   \n",
       "...                                                 ...   \n",
       "4354  Do G1 PA A situa√ß√£o da tuberculose no Par√° foi...   \n",
       "4355  Do G1 Campinas e Regi√£o  O Hospital e Maternid...   \n",
       "4357  Do G1, com informa√ß√µes da Reuters O n√∫mero de ...   \n",
       "4358  Reuters (Reuters) - O n√∫mero de pessoas que ad...   \n",
       "4361  Do G1 Campinas e Regi√£o O n√∫mero de beb√™s infe...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     paulo anvisa anunciar feira suspens√£o antibi√≥t...  \n",
       "1     rjtv conte√∫do dispon√≠vel infelizmente v√≠deo di...  \n",
       "2     kleber tomaz paulo hatualizadoh√° ano antibi√≥ti...  \n",
       "7     paulo ag√™ncia nacional vigil√¢ncia sanit√°ria an...  \n",
       "8     ribeir√£o franca jovem ano aprender instrumento...  \n",
       "...                                                 ...  \n",
       "4354  situa√ß√£o tuberculose par tema reuni√£o secretar...  \n",
       "4355  campina regi√£o hospital maternidade madre theo...  \n",
       "4357  informa√ß√£o reuter pessoa doente conta tubercul...  \n",
       "4358  reuter reuters pessoa adoecer tuberculose cair...  \n",
       "4361  campina regi√£o beb√™ infectar tuberculose hospi...  \n",
       "\n",
       "[1999 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139182f",
   "metadata": {},
   "source": [
    "Se quiser salvar como Xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf765f50",
   "metadata": {},
   "source": [
    "# Esse trecho se dedica a tratar os trechos para Not√≠cias do GOV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79f5ce",
   "metadata": {},
   "source": [
    "### Carregar planilha e selecionar colunas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5dbff837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Titulo', 'Descricao', 'Link', 'Conteudo_Materia'], dtype='object')\n",
      "['Titulo', 'Descricao', 'Link', 'Conteudo_Materia']\n",
      "                                                 Titulo  \\\n",
      "0     Comunidades ind√≠genas do Vale do Javari ganham...   \n",
      "1     Mutir√£o leva atendimento m√©dico e insumos ao V...   \n",
      "2     Governo brasileiro envia insumos e medicamento...   \n",
      "3     Reajuste de medicamentos √© o menor registrado ...   \n",
      "4     DSEI Alto Rio Juru√° recebe medicamentos da Ren...   \n",
      "...                                                 ...   \n",
      "1497  DSEI Kaiap√≥ do Par√° articula parceria com hosp...   \n",
      "1499  DSEI M√©dio Rio Solim√µes realiza a√ß√£o de sa√∫de ...   \n",
      "1501  DSEI Vilhena promove capacita√ß√£o para profissi...   \n",
      "1503  DSEI Parintins participa de mostra em parceria...   \n",
      "1505  DSEI Kayap√≥ Mato Grosso realiza a√ß√£o sobre cui...   \n",
      "\n",
      "                                       Conteudo_Materia  \n",
      "0     Comunidades da Terra Ind√≠gena do Vale do Javar...  \n",
      "1     O Governo Federal vai realizar um novo mutir√£o...  \n",
      "2     Pa√≠s africano afetado por ciclone vai receber ...  \n",
      "3     Em 2017, o ajuste m√©dio autorizado pela C√¢mara...  \n",
      "4     Na √∫ltima segunda-feira (24), o Distrito Sanit...  \n",
      "...                                                 ...  \n",
      "1497  Na √∫ltima semana, o Distrito Sanit√°rio Especia...  \n",
      "1499  As equipes¬†da Divis√£o de Aten√ß√£o √† Sa√∫de Ind√≠g...  \n",
      "1501  Termina nesta quinta-feira (29) a capacita√ß√£o ...  \n",
      "1503  O Distrito Sanit√°rio Especial Ind√≠gena (DSEI) ...  \n",
      "1505  O Programa de Controle daTuberculosedo Distrit...  \n",
      "\n",
      "[1224 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\dados\\dadosGov\\unificado\\unificado.csv'\n",
    "news_df = pd.read_csv(caminho_arquivo, delimiter=',', quotechar='\"')\n",
    "print(news_df.columns)\n",
    "print(list(news_df.columns))\n",
    "\n",
    "\n",
    "#Escolhendo as colunas relevantes\n",
    "news_df = news_df[['Titulo', 'Conteudo_Materia']]\n",
    "\n",
    "\n",
    "#S√≥ por precau√ß√£o, removendo duplicatas\n",
    "news_df.drop_duplicates(inplace=True, subset=\"Titulo\")\n",
    "news_df.drop_duplicates(inplace=True, subset=\"Conteudo_Materia\")\n",
    "\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "66db930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been tokenized : 1223\n",
      "Arquivo 'noticias_g1_processados.csv' salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "news_df = news_df.dropna(subset=[\"Conteudo_Materia\"])\n",
    "news_df = tokenize_gov(news_df)\n",
    "\n",
    "#Removendo tweets com menos de 6 palavras\n",
    "news_df[\"word_count\"] = news_df[\"tokens\"].apply(lambda x: len(x.split()))\n",
    "news_df = news_df[news_df[\"word_count\"] >= 6]\n",
    "news_df = news_df.drop(columns=[\"word_count\"])\n",
    "news_df.drop_duplicates(inplace=True, subset=\"tokens\")\n",
    "news_df.to_csv(\"news_processados.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Arquivo 'noticias_g1_processados.csv' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bc6783d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'news_processados.xlsx' salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "news_df.to_excel(\"news_processados.xlsx\", index=False)\n",
    "print(\"Arquivo 'news_processados.xlsx' salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7984b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Conteudo_Materia</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pesquisa in√©dita financiada pelo Minist√©rio da...</td>\n",
       "      <td>O Minist√©rio da Sa√∫de financiou um estudo in√©d...</td>\n",
       "      <td>minist√©rio sa√∫de financiar estudo in√©dito prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPSUS em Roraima ter√° pesquisas voltadas para ...</td>\n",
       "      <td>O Minist√©rio da Sa√∫de, por meio do Departament...</td>\n",
       "      <td>minist√©rio sa√∫de departamento ci√™ncia tecnolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brasil amplia preven√ß√£o e aposta em tratamento...</td>\n",
       "      <td>O Brasil vem intensificando os esfor√ßos para e...</td>\n",
       "      <td>brasil intensificar esfor√ßo eliminar atubercul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brasil firma acordo com S√£o Tom√© e Pr√≠ncipe pa...</td>\n",
       "      <td>Brasil e S√£o Tom√© e Pr√≠ncipe v√£o desenvolver u...</td>\n",
       "      <td>brasil tom√© pr√≠ncipe desenvolver amplo coopera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sa√∫de lan√ßa painel de doen√ßas e agravos na pop...</td>\n",
       "      <td>OCentro Nacional de Intelig√™ncia Epidemiol√≥gic...</td>\n",
       "      <td>ocentro nacional intelig√™ncia epidemiol√≥gico v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>DSEI Kaiap√≥ do Par√° articula parceria com hosp...</td>\n",
       "      <td>Na √∫ltima semana, o Distrito Sanit√°rio Especia...</td>\n",
       "      <td>√∫ltimo semana distrito sanit√°rio especial ind√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>DSEI M√©dio Rio Solim√µes realiza a√ß√£o de sa√∫de ...</td>\n",
       "      <td>As equipes¬†da Divis√£o de Aten√ß√£o √† Sa√∫de Ind√≠g...</td>\n",
       "      <td>equipe divis√£o aten√ß√£o sa√∫de ind√≠geno diasi se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>DSEI Vilhena promove capacita√ß√£o para profissi...</td>\n",
       "      <td>Termina nesta quinta-feira (29) a capacita√ß√£o ...</td>\n",
       "      <td>terminar feira capacita√ß√£o emtuberculose hanse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>DSEI Parintins participa de mostra em parceria...</td>\n",
       "      <td>O Distrito Sanit√°rio Especial Ind√≠gena (DSEI) ...</td>\n",
       "      <td>distrito sanit√°rio especial ind√≠gena dsar pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>DSEI Kayap√≥ Mato Grosso realiza a√ß√£o sobre cui...</td>\n",
       "      <td>O Programa de Controle daTuberculosedo Distrit...</td>\n",
       "      <td>programa controle datuberculosedo distrito san...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Titulo  \\\n",
       "0    Pesquisa in√©dita financiada pelo Minist√©rio da...   \n",
       "1    PPSUS em Roraima ter√° pesquisas voltadas para ...   \n",
       "2    Brasil amplia preven√ß√£o e aposta em tratamento...   \n",
       "3    Brasil firma acordo com S√£o Tom√© e Pr√≠ncipe pa...   \n",
       "4    Sa√∫de lan√ßa painel de doen√ßas e agravos na pop...   \n",
       "..                                                 ...   \n",
       "781  DSEI Kaiap√≥ do Par√° articula parceria com hosp...   \n",
       "783  DSEI M√©dio Rio Solim√µes realiza a√ß√£o de sa√∫de ...   \n",
       "785  DSEI Vilhena promove capacita√ß√£o para profissi...   \n",
       "787  DSEI Parintins participa de mostra em parceria...   \n",
       "789  DSEI Kayap√≥ Mato Grosso realiza a√ß√£o sobre cui...   \n",
       "\n",
       "                                      Conteudo_Materia  \\\n",
       "0    O Minist√©rio da Sa√∫de financiou um estudo in√©d...   \n",
       "1    O Minist√©rio da Sa√∫de, por meio do Departament...   \n",
       "2    O Brasil vem intensificando os esfor√ßos para e...   \n",
       "3    Brasil e S√£o Tom√© e Pr√≠ncipe v√£o desenvolver u...   \n",
       "4    OCentro Nacional de Intelig√™ncia Epidemiol√≥gic...   \n",
       "..                                                 ...   \n",
       "781  Na √∫ltima semana, o Distrito Sanit√°rio Especia...   \n",
       "783  As equipes¬†da Divis√£o de Aten√ß√£o √† Sa√∫de Ind√≠g...   \n",
       "785  Termina nesta quinta-feira (29) a capacita√ß√£o ...   \n",
       "787  O Distrito Sanit√°rio Especial Ind√≠gena (DSEI) ...   \n",
       "789  O Programa de Controle daTuberculosedo Distrit...   \n",
       "\n",
       "                                                tokens  \n",
       "0    minist√©rio sa√∫de financiar estudo in√©dito prog...  \n",
       "1    minist√©rio sa√∫de departamento ci√™ncia tecnolog...  \n",
       "2    brasil intensificar esfor√ßo eliminar atubercul...  \n",
       "3    brasil tom√© pr√≠ncipe desenvolver amplo coopera...  \n",
       "4    ocentro nacional intelig√™ncia epidemiol√≥gico v...  \n",
       "..                                                 ...  \n",
       "781  √∫ltimo semana distrito sanit√°rio especial ind√≠...  \n",
       "783  equipe divis√£o aten√ß√£o sa√∫de ind√≠geno diasi se...  \n",
       "785  terminar feira capacita√ß√£o emtuberculose hanse...  \n",
       "787  distrito sanit√°rio especial ind√≠gena dsar pari...  \n",
       "789  programa controle datuberculosedo distrito san...  \n",
       "\n",
       "[732 rows x 3 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
