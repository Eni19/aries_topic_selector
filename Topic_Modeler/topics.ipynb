{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ce902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9194aae",
   "metadata": {},
   "source": [
    "Ler o CSV criado previamente e converter tokens em listas de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da489b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\783247071.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweets_df['tokens'] = tweets_df.tokens.apply(lambda x:  re.split('\\s', x))\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\Noise_remover\\tweets_processados.csv'\n",
    "tweets_df = pd.read_csv(caminho_arquivo)\n",
    "tweets_df.dropna(axis='columns', inplace=True)\n",
    "tweets_df['tokens'] = tweets_df.tokens.apply(lambda x:  re.split('\\s', x))\n",
    "docs = tweets_df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1da713",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo usando o GSDMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97a321",
   "metadata": {},
   "source": [
    "Agora, aplicaremos o GSDMM (Gibbs Sampling Dirichlet Mixture Model) , com os seguintes parâmetros:\n",
    "- K — É o número máximo de topicos a serem encontrados\n",
    "- Alpha α - Determina o quão fácil mesas pequenas são removidas\n",
    "- Beta β - Controla se uma mesa é escolhida por popularidade ou similaridade. Quanto menor, mais popular\n",
    "- N_iters - Número de vezes em que um Subject é realocado numa nova mesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c643aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\3350084293.py:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 16020 clusters with 10 clusters populated\n",
      "In stage 1: transferred 10844 clusters with 10 clusters populated\n",
      "In stage 2: transferred 6986 clusters with 10 clusters populated\n",
      "In stage 3: transferred 4953 clusters with 10 clusters populated\n",
      "In stage 4: transferred 3878 clusters with 10 clusters populated\n",
      "In stage 5: transferred 3401 clusters with 10 clusters populated\n",
      "In stage 6: transferred 2853 clusters with 10 clusters populated\n",
      "In stage 7: transferred 2565 clusters with 10 clusters populated\n",
      "In stage 8: transferred 2384 clusters with 10 clusters populated\n",
      "In stage 9: transferred 2275 clusters with 10 clusters populated\n",
      "In stage 10: transferred 2137 clusters with 10 clusters populated\n",
      "In stage 11: transferred 2093 clusters with 10 clusters populated\n",
      "In stage 12: transferred 2067 clusters with 10 clusters populated\n",
      "In stage 13: transferred 1975 clusters with 10 clusters populated\n",
      "In stage 14: transferred 2037 clusters with 10 clusters populated\n",
      "In stage 15: transferred 2027 clusters with 10 clusters populated\n",
      "In stage 16: transferred 1962 clusters with 10 clusters populated\n",
      "In stage 17: transferred 1966 clusters with 10 clusters populated\n",
      "In stage 18: transferred 1972 clusters with 10 clusters populated\n",
      "In stage 19: transferred 1925 clusters with 10 clusters populated\n",
      "In stage 20: transferred 1854 clusters with 10 clusters populated\n",
      "In stage 21: transferred 1903 clusters with 10 clusters populated\n",
      "In stage 22: transferred 1951 clusters with 10 clusters populated\n",
      "In stage 23: transferred 1898 clusters with 10 clusters populated\n",
      "In stage 24: transferred 1892 clusters with 10 clusters populated\n",
      "In stage 25: transferred 1847 clusters with 10 clusters populated\n",
      "In stage 26: transferred 1984 clusters with 10 clusters populated\n",
      "In stage 27: transferred 1955 clusters with 10 clusters populated\n",
      "In stage 28: transferred 1961 clusters with 10 clusters populated\n",
      "In stage 29: transferred 1867 clusters with 10 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(docs, n_terms)\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec9e2b",
   "metadata": {},
   "source": [
    "# Explorando os Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca277124",
   "metadata": {},
   "source": [
    "Definiremos algumas funções para ajudar a explorar os tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426b42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    '''prints the top words in each cluster'''\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print(' — — — — — — — — —')\n",
    "\n",
    "def cluster_importance(mgp):\n",
    "    '''returns a word-topic matrix[phi] where each value represents\n",
    "    the word importance for that particular cluster;\n",
    "    phi[i][w] would be the importance of word w in topic i.\n",
    "    '''\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]\n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi\n",
    "\n",
    "def topic_allocation(df, docs, mgp, topic_dict):\n",
    "    '''allocates all topics to each document in original dataframe,\n",
    "    adding two columns for cluster number and cluster description'''\n",
    "    topic_allocations = []\n",
    "    for doc in tqdm(docs):\n",
    "        topic_label, score = mgp.choose_best_label(doc)\n",
    "        topic_allocations.append(topic_label)\n",
    "\n",
    "    df['cluster'] = topic_allocations\n",
    "\n",
    "    df['topic_name'] = df.cluster.apply(lambda x: get_topic_name(x, topic_dict))\n",
    "    print('Complete. Number of documents with topic allocated: {}'.format(len(df)))\n",
    "\n",
    "def get_topic_name(doc, topic_dict):\n",
    "    '''returns the topic name string value from a dictionary of topics'''\n",
    "    topic_desc = topic_dict[doc]\n",
    "    return topic_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec576b7b",
   "metadata": {},
   "source": [
    "Insight estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26154742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 106 1211 2728  938 4068 1238  246 1704 7105  227]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [8 4 2 7 5 1 3 6 9 0]\n",
      "********************\n",
      "Cluster 0 : [('amoxicilino', 27), ('tuberculose', 22), ('água', 14), ('amoxicilina', 12), ('remédio', 10)]\n",
      " — — — — — — — — —\n",
      "Cluster 1 : [('infecção', 789), ('urinário', 601), ('tuberculose', 366), ('xixi', 155), ('doença', 145)]\n",
      " — — — — — — — — —\n",
      "Cluster 2 : [('bactéria', 2126), ('antibiótico', 1702), ('resistência', 1190), ('resistente', 1006), ('tomar', 488)]\n",
      " — — — — — — — — —\n",
      "Cluster 3 : [('antibiotico', 505), ('che', 336), ('non', 334), ('tuberculose', 317), ('per', 262)]\n",
      " — — — — — — — — —\n",
      "Cluster 4 : [('antibiótico', 1834), ('infecção', 1282), ('urinário', 859), ('médico', 829), ('tomar', 620)]\n",
      " — — — — — — — — —\n",
      "Cluster 5 : [('tuberculose', 1224), ('doença', 360), ('saúde', 240), ('tratamento', 224), ('caso', 173)]\n",
      " — — — — — — — — —\n",
      "Cluster 6 : [('una', 56), ('atendimento', 54), ('pré', 52), ('hospitalar', 52), ('antibiótico', 48)]\n",
      " — — — — — — — — —\n",
      "Cluster 7 : [('tuberculose', 735), ('nao', 248), ('morrer', 244), ('ir', 193), ('arthur', 153)]\n",
      " — — — — — — — — —\n",
      "Cluster 8 : [('antibiótico', 3337), ('tomar', 2529), ('infecção', 2269), ('urinário', 1720), ('ir', 1131)]\n",
      " — — — — — — — — —\n",
      "Cluster 9 : [('infecção', 187), ('urinário', 167), ('gil', 99), ('dar', 80), ('preto', 73)]\n",
      " — — — — — — — — —\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):',top_index)\n",
    "print('*'*20)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde7550",
   "metadata": {},
   "source": [
    "Se quisermos ver a importância de algumas palavras , usando a função cluster_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becab96c",
   "metadata": {},
   "source": [
    "Agora, decidimos o que é  cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc671715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19571/19571 [00:06<00:00, 3124.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of documents with topic allocated: 19571\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Cluster 0,',\n",
    "                'Cluster 1,',\n",
    "                'Cluster 2,',\n",
    "                'Cluster 3,',\n",
    "                'Cluster 4,',\n",
    "                'Cluster 5,',\n",
    "                'Cluster 6,',\n",
    "                'Cluster 7,',\n",
    "                'Cluster 8,',\n",
    "                'Cluster 9,']\n",
    "\n",
    "for i, topic_num in enumerate(topic_indices):\n",
    " topic_dict[topic_num]=topic_names[i]\n",
    "\n",
    "\n",
    "\n",
    "topic_allocation(tweets_df, docs, mgp, topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f1b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_excel('sttm_10topics_results.xlsx', index = False,  header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e227ec",
   "metadata": {},
   "source": [
    "# Aplicação do Método para Notícias do G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b2ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\3873121176.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  news_df['tokens'] = news_df.tokens.apply(lambda x:  re.split('\\s', x))\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\Noise_remover\\noticias_processados.csv'\n",
    "news_df = pd.read_csv(caminho_arquivo)\n",
    "news_df.dropna(axis='columns', inplace=True)\n",
    "news_df['tokens'] = news_df.tokens.apply(lambda x:  re.split('\\s', x))\n",
    "docs = news_df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e94284",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo usando o GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a00ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\2481954221.py:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 168 clusters with 9 clusters populated\n",
      "In stage 1: transferred 23 clusters with 6 clusters populated\n",
      "In stage 2: transferred 6 clusters with 6 clusters populated\n",
      "In stage 3: transferred 1 clusters with 6 clusters populated\n",
      "In stage 4: transferred 0 clusters with 6 clusters populated\n",
      "In stage 5: transferred 2 clusters with 6 clusters populated\n",
      "In stage 6: transferred 1 clusters with 6 clusters populated\n",
      "In stage 7: transferred 0 clusters with 6 clusters populated\n",
      "In stage 8: transferred 1 clusters with 6 clusters populated\n",
      "In stage 9: transferred 0 clusters with 6 clusters populated\n",
      "In stage 10: transferred 1 clusters with 6 clusters populated\n",
      "In stage 11: transferred 1 clusters with 6 clusters populated\n",
      "In stage 12: transferred 0 clusters with 6 clusters populated\n",
      "In stage 13: transferred 2 clusters with 6 clusters populated\n",
      "In stage 14: transferred 1 clusters with 6 clusters populated\n",
      "In stage 15: transferred 1 clusters with 6 clusters populated\n",
      "In stage 16: transferred 1 clusters with 6 clusters populated\n",
      "In stage 17: transferred 1 clusters with 6 clusters populated\n",
      "In stage 18: transferred 1 clusters with 6 clusters populated\n",
      "In stage 19: transferred 1 clusters with 6 clusters populated\n",
      "In stage 20: transferred 0 clusters with 6 clusters populated\n",
      "In stage 21: transferred 0 clusters with 6 clusters populated\n",
      "In stage 22: transferred 0 clusters with 6 clusters populated\n",
      "In stage 23: transferred 0 clusters with 6 clusters populated\n",
      "In stage 24: transferred 2 clusters with 6 clusters populated\n",
      "In stage 25: transferred 2 clusters with 6 clusters populated\n",
      "In stage 26: transferred 1 clusters with 6 clusters populated\n",
      "In stage 27: transferred 2 clusters with 6 clusters populated\n",
      "In stage 28: transferred 1 clusters with 6 clusters populated\n",
      "In stage 29: transferred 1 clusters with 6 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp2 = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp2.fit(docs, n_terms)\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n",
    "    pickle.dump(mgp2, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80397b",
   "metadata": {},
   "source": [
    "Insight estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb77d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [  0   0   0  17  13   0   4   0  11 139]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [9 3 4 8 6 7 5 2 1 0]\n",
      "********************\n",
      "Cluster 0 : []\n",
      " — — — — — — — — —\n",
      "Cluster 1 : []\n",
      " — — — — — — — — —\n",
      "Cluster 2 : []\n",
      " — — — — — — — — —\n",
      "Cluster 3 : [('magaziner', 42), ('luizar', 33), ('hospital', 32), ('fnac', 30), ('ano', 21)]\n",
      " — — — — — — — — —\n",
      "Cluster 4 : [('magaziner', 34), ('luizar', 28), ('hospital', 27), ('fnac', 26), ('chico', 23)]\n",
      " — — — — — — — — —\n",
      "Cluster 5 : []\n",
      " — — — — — — — — —\n",
      "Cluster 6 : [('magaziner', 9), ('luizar', 7), ('rio', 7), ('feira', 7), ('ano', 6)]\n",
      " — — — — — — — — —\n",
      "Cluster 7 : []\n",
      " — — — — — — — — —\n",
      "Cluster 8 : [('magaziner', 30), ('luizar', 24), ('fnac', 22), ('pneumonia', 19), ('feira', 15)]\n",
      " — — — — — — — — —\n",
      "Cluster 9 : [('pneumonia', 328), ('ano', 280), ('hospital', 253), ('magaziner', 244), ('médico', 233)]\n",
      " — — — — — — — — —\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):',top_index)\n",
    "print('*'*20)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4268/4268 [00:01<00:00, 3300.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of documents with topic allocated: 4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Cluster 0,',\n",
    "                'Cluster 1,',\n",
    "                'Cluster 2,',\n",
    "                'Cluster 3,',\n",
    "                'Cluster 4,',\n",
    "                'Cluster 5,',\n",
    "                'Cluster 6,',\n",
    "                'Cluster 7,',\n",
    "                'Cluster 8,',\n",
    "                'Cluster 9,']\n",
    "\n",
    "for i, topic_num in enumerate(topic_indices):\n",
    " topic_dict[topic_num]=topic_names[i]\n",
    "\n",
    "\n",
    "\n",
    "topic_allocation(tweets_df, docs, mgp, topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_excel('sttm_10topics_results.xlsx', index = False,  header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
