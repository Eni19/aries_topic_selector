{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ce902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9194aae",
   "metadata": {},
   "source": [
    "Ler o CSV criado previamente e converter tokens em listas de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da489b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_9596\\3829233819.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tweets_df['tokens'] = tweets_df.tokens.apply(lambda x:  re.split('\\s', x))\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\dados\\dadosTwitter\\CSV\\resitencia_bacteria\\tweets_processados.csv'\n",
    "tweets_df = pd.read_csv(caminho_arquivo)\n",
    "tweets_df.dropna(axis='columns', inplace=True)\n",
    "tweets_df['tokens'] = tweets_df.tokens.apply(lambda x:  re.split('\\s', x))\n",
    "docs = tweets_df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1da713",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo usando o GSDMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97a321",
   "metadata": {},
   "source": [
    "Agora, aplicaremos o GSDMM (Gibbs Sampling Dirichlet Mixture Model) , com os seguintes parâmetros:\n",
    "- K — É o número máximo de topicos a serem encontrados\n",
    "- Alpha α - Determina o quão fácil mesas pequenas são removidas\n",
    "- Beta β - Controla se uma mesa é escolhida por popularidade ou similaridade. Quanto menor, mais popular\n",
    "- N_iters - Número de vezes em que um Subject é realocado numa nova mesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c643aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_9596\\993567298.py:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 954 clusters with 15 clusters populated\n",
      "In stage 1: transferred 578 clusters with 15 clusters populated\n",
      "In stage 2: transferred 403 clusters with 15 clusters populated\n",
      "In stage 3: transferred 336 clusters with 15 clusters populated\n",
      "In stage 4: transferred 255 clusters with 15 clusters populated\n",
      "In stage 5: transferred 220 clusters with 15 clusters populated\n",
      "In stage 6: transferred 228 clusters with 15 clusters populated\n",
      "In stage 7: transferred 185 clusters with 15 clusters populated\n",
      "In stage 8: transferred 177 clusters with 15 clusters populated\n",
      "In stage 9: transferred 186 clusters with 15 clusters populated\n",
      "In stage 10: transferred 187 clusters with 15 clusters populated\n",
      "In stage 11: transferred 185 clusters with 15 clusters populated\n",
      "In stage 12: transferred 171 clusters with 15 clusters populated\n",
      "In stage 13: transferred 178 clusters with 15 clusters populated\n",
      "In stage 14: transferred 164 clusters with 15 clusters populated\n",
      "In stage 15: transferred 162 clusters with 15 clusters populated\n",
      "In stage 16: transferred 182 clusters with 15 clusters populated\n",
      "In stage 17: transferred 159 clusters with 15 clusters populated\n",
      "In stage 18: transferred 163 clusters with 15 clusters populated\n",
      "In stage 19: transferred 164 clusters with 15 clusters populated\n",
      "In stage 20: transferred 165 clusters with 15 clusters populated\n",
      "In stage 21: transferred 155 clusters with 15 clusters populated\n",
      "In stage 22: transferred 156 clusters with 15 clusters populated\n",
      "In stage 23: transferred 168 clusters with 15 clusters populated\n",
      "In stage 24: transferred 162 clusters with 15 clusters populated\n",
      "In stage 25: transferred 155 clusters with 15 clusters populated\n",
      "In stage 26: transferred 176 clusters with 15 clusters populated\n",
      "In stage 27: transferred 167 clusters with 15 clusters populated\n",
      "In stage 28: transferred 159 clusters with 15 clusters populated\n",
      "In stage 29: transferred 170 clusters with 15 clusters populated\n",
      "In stage 30: transferred 156 clusters with 15 clusters populated\n",
      "In stage 31: transferred 161 clusters with 15 clusters populated\n",
      "In stage 32: transferred 137 clusters with 15 clusters populated\n",
      "In stage 33: transferred 135 clusters with 15 clusters populated\n",
      "In stage 34: transferred 128 clusters with 15 clusters populated\n",
      "In stage 35: transferred 149 clusters with 15 clusters populated\n",
      "In stage 36: transferred 154 clusters with 15 clusters populated\n",
      "In stage 37: transferred 157 clusters with 15 clusters populated\n",
      "In stage 38: transferred 157 clusters with 15 clusters populated\n",
      "In stage 39: transferred 148 clusters with 15 clusters populated\n",
      "In stage 40: transferred 143 clusters with 15 clusters populated\n",
      "In stage 41: transferred 154 clusters with 15 clusters populated\n",
      "In stage 42: transferred 153 clusters with 15 clusters populated\n",
      "In stage 43: transferred 153 clusters with 15 clusters populated\n",
      "In stage 44: transferred 142 clusters with 15 clusters populated\n",
      "In stage 45: transferred 143 clusters with 15 clusters populated\n",
      "In stage 46: transferred 168 clusters with 15 clusters populated\n",
      "In stage 47: transferred 124 clusters with 15 clusters populated\n",
      "In stage 48: transferred 144 clusters with 15 clusters populated\n",
      "In stage 49: transferred 160 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=15, alpha=0.08, beta=0.1, n_iters=50)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(docs, n_terms)\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec9e2b",
   "metadata": {},
   "source": [
    "# Explorando os Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca277124",
   "metadata": {},
   "source": [
    "Definiremos algumas funções para ajudar a explorar os tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426b42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    '''prints the top words in each cluster'''\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print(' — — — — — — — — —')\n",
    "\n",
    "def cluster_importance(mgp):\n",
    "    '''returns a word-topic matrix[phi] where each value represents\n",
    "    the word importance for that particular cluster;\n",
    "    phi[i][w] would be the importance of word w in topic i.\n",
    "    '''\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]\n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi\n",
    "\n",
    "def topic_allocation(df, docs, mgp, topic_dict):\n",
    "    '''allocates all topics to each document in original dataframe,\n",
    "    adding two columns for cluster number and cluster description'''\n",
    "    topic_allocations = []\n",
    "    for doc in tqdm(docs):\n",
    "        topic_label, score = mgp.choose_best_label(doc)\n",
    "        topic_allocations.append(topic_label)\n",
    "\n",
    "    df['cluster'] = topic_allocations\n",
    "\n",
    "    df['topic_name'] = df.cluster.apply(lambda x: get_topic_name(x, topic_dict))\n",
    "    print('Complete. Number of documents with topic allocated: {}'.format(len(df)))\n",
    "\n",
    "def get_topic_name(doc, topic_dict):\n",
    "    '''returns the topic name string value from a dictionary of topics'''\n",
    "    topic_desc = topic_dict[doc]\n",
    "    return topic_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec576b7b",
   "metadata": {},
   "source": [
    "Insight estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26154742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 48  53  54  32  23  67  30  33  29  19  94 360  45  36 316]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [11 14 10  5  2  1  0 12 13  7]\n",
      "********************\n",
      "Cluster 0 : [('criar', 15), ('bolsonaro', 8), ('ser', 7), ('forte', 7), ('brasil', 7)]\n",
      " — — — — — — — — —\n",
      "Cluster 1 : [('infecção', 13), ('tratamento', 12), ('desenvolver', 10), ('tratar', 8), ('ério', 8)]\n",
      " — — — — — — — — —\n",
      "Cluster 2 : [('mecanismo', 12), ('ser', 11), ('gene', 10), ('parede', 9), ('celular', 9)]\n",
      " — — — — — — — — —\n",
      "Cluster 3 : [('una', 20), ('resistència', 14), ('per', 13), ('als', 9), ('antibiòtics', 7)]\n",
      " — — — — — — — — —\n",
      "Cluster 4 : [('gene', 8), ('coisa', 6), ('humano', 4), ('lagarta', 3), ('soja', 3)]\n",
      " — — — — — — — — —\n",
      "Cluster 5 : [('gene', 12), ('reduzir', 10), ('estudo', 9), ('base', 8), ('infecção', 8)]\n",
      " — — — — — — — — —\n",
      "Cluster 6 : [('anel', 5), ('outro', 5), ('de', 5), ('ocorrer', 4), ('tratar', 4)]\n",
      " — — — — — — — — —\n",
      "Cluster 7 : [('alto', 11), ('hospital', 10), ('uti', 8), ('er', 6), ('neonatal', 6)]\n",
      " — — — — — — — — —\n",
      "Cluster 8 : [('problema', 10), ('antimicrobiano', 4), ('hospital', 4), ('esperar', 4), ('ir', 4)]\n",
      " — — — — — — — — —\n",
      "Cluster 9 : [('prova', 5), ('criar', 4), ('banho', 4), ('virar', 3), ('burro', 3)]\n",
      " — — — — — — — — —\n",
      "Cluster 10 : [('criar', 31), ('tomar', 14), ('falar', 14), ('passar', 14), ('gente', 12)]\n",
      " — — — — — — — — —\n",
      "Cluster 11 : [('tomar', 170), ('criar', 167), ('ficar', 58), ('remédio', 50), ('ter', 49)]\n",
      " — — — — — — — — —\n",
      "Cluster 12 : [('de', 12), ('efeito', 12), ('medicamento', 12), ('em', 8), ('criar', 6)]\n",
      " — — — — — — — — —\n",
      "Cluster 13 : [('caso', 7), ('microrganismo', 6), ('ano', 5), ('klebsiella', 5), ('pneumoniae', 5)]\n",
      " — — — — — — — — —\n",
      "Cluster 14 : [('criar', 89), ('uso', 68), ('vírus', 61), ('ser', 57), ('medicamento', 44)]\n",
      " — — — — — — — — —\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):',top_index)\n",
    "print('*'*20)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde7550",
   "metadata": {},
   "source": [
    "Se quisermos ver a importância de algumas palavras , usando a função cluster_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becab96c",
   "metadata": {},
   "source": [
    "Agora, decidimos o que é  cada tópico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7f8e1",
   "metadata": {},
   "source": [
    "# metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8db1c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GSDMM] Coerência (C_V): 0.3471\n",
      "[GSDMM] Diversidade: 0.7467\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "# Supondo que:\n",
    "# - mgp é o modelo treinado com fit()\n",
    "# - docs é uma lista de listas de tokens (tokenized_docs)\n",
    "docs = tweets_df[\"tokens\"].tolist()\n",
    "# Obter tópicos (palavras mais frequentes por cluster)\n",
    "def extract_top_words(cluster_word_dist, top_n=10):\n",
    "    topic_words = []\n",
    "    for cluster_id in range(len(cluster_word_dist)):\n",
    "        sorted_words = sorted(cluster_word_dist[cluster_id].items(), key=lambda kv: kv[1], reverse=True)\n",
    "        words = [word for word, _ in sorted_words[:top_n]]\n",
    "        topic_words.append(words)\n",
    "    return topic_words\n",
    "\n",
    "# Extrair palavras por tópico\n",
    "top_n = 10\n",
    "topic_words = extract_top_words(mgp.cluster_word_distribution, top_n=top_n)\n",
    "\n",
    "# Remover tópicos vazios (podem acontecer se o cluster não tiver documentos)\n",
    "topic_words = [topic for topic in topic_words if len(topic) > 0]\n",
    "\n",
    "# Criar dicionário e corpus no formato Gensim\n",
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "\n",
    "# Calcular Coerência\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Calcular Diversidade\n",
    "all_words = [word for topic in topic_words for word in topic]\n",
    "unique_words = set(all_words)\n",
    "diversity_score = len(unique_words) / len(all_words)\n",
    "\n",
    "# Resultados\n",
    "print(f\"[GSDMM] Coerência (C_V): {coherence_score:.4f}\")\n",
    "print(f\"[GSDMM] Diversidade: {diversity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc671715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19571/19571 [00:06<00:00, 3124.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of documents with topic allocated: 19571\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Cluster 0,',\n",
    "                'Cluster 1,',\n",
    "                'Cluster 2,',\n",
    "                'Cluster 3,',\n",
    "                'Cluster 4,',\n",
    "                'Cluster 5,',\n",
    "                'Cluster 6,',\n",
    "                'Cluster 7,',\n",
    "                'Cluster 8,',\n",
    "                'Cluster 9,']\n",
    "\n",
    "for i, topic_num in enumerate(topic_indices):\n",
    " topic_dict[topic_num]=topic_names[i]\n",
    "\n",
    "\n",
    "\n",
    "topic_allocation(tweets_df, docs, mgp, topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f1b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_excel('sttm_10topics_results.xlsx', index = False,  header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e227ec",
   "metadata": {},
   "source": [
    "# Aplicação do Método para Notícias do G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b2ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\3873121176.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  news_df['tokens'] = news_df.tokens.apply(lambda x:  re.split('\\s', x))\n"
     ]
    }
   ],
   "source": [
    "caminho_arquivo = r'C:\\Users\\Pessoal\\Documents\\Unifesp\\Aries\\aries_topic_selector\\Noise_remover\\noticias_processados.csv'\n",
    "news_df = pd.read_csv(caminho_arquivo)\n",
    "news_df.dropna(axis='columns', inplace=True)\n",
    "news_df['tokens'] = news_df.tokens.apply(lambda x:  re.split('\\s', x))\n",
    "docs = news_df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e94284",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo usando o GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a00ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\Pessoal\\AppData\\Local\\Temp\\ipykernel_12864\\2481954221.py:7: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 168 clusters with 9 clusters populated\n",
      "In stage 1: transferred 23 clusters with 6 clusters populated\n",
      "In stage 2: transferred 6 clusters with 6 clusters populated\n",
      "In stage 3: transferred 1 clusters with 6 clusters populated\n",
      "In stage 4: transferred 0 clusters with 6 clusters populated\n",
      "In stage 5: transferred 2 clusters with 6 clusters populated\n",
      "In stage 6: transferred 1 clusters with 6 clusters populated\n",
      "In stage 7: transferred 0 clusters with 6 clusters populated\n",
      "In stage 8: transferred 1 clusters with 6 clusters populated\n",
      "In stage 9: transferred 0 clusters with 6 clusters populated\n",
      "In stage 10: transferred 1 clusters with 6 clusters populated\n",
      "In stage 11: transferred 1 clusters with 6 clusters populated\n",
      "In stage 12: transferred 0 clusters with 6 clusters populated\n",
      "In stage 13: transferred 2 clusters with 6 clusters populated\n",
      "In stage 14: transferred 1 clusters with 6 clusters populated\n",
      "In stage 15: transferred 1 clusters with 6 clusters populated\n",
      "In stage 16: transferred 1 clusters with 6 clusters populated\n",
      "In stage 17: transferred 1 clusters with 6 clusters populated\n",
      "In stage 18: transferred 1 clusters with 6 clusters populated\n",
      "In stage 19: transferred 1 clusters with 6 clusters populated\n",
      "In stage 20: transferred 0 clusters with 6 clusters populated\n",
      "In stage 21: transferred 0 clusters with 6 clusters populated\n",
      "In stage 22: transferred 0 clusters with 6 clusters populated\n",
      "In stage 23: transferred 0 clusters with 6 clusters populated\n",
      "In stage 24: transferred 2 clusters with 6 clusters populated\n",
      "In stage 25: transferred 2 clusters with 6 clusters populated\n",
      "In stage 26: transferred 1 clusters with 6 clusters populated\n",
      "In stage 27: transferred 2 clusters with 6 clusters populated\n",
      "In stage 28: transferred 1 clusters with 6 clusters populated\n",
      "In stage 29: transferred 1 clusters with 6 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp2 = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp2.fit(docs, n_terms)\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Pessoal\\\\Documents\\\\Unifesp\\Aries\\\\aries_topic_selector\\\\modelos_treinados\\\\10clusters.model\", 'wb') as f:\n",
    "    pickle.dump(mgp2, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80397b",
   "metadata": {},
   "source": [
    "Insight estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb77d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 48  53  54  32  23  67  30  33  29  19  94 360  45  36 316]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [11 14 10  5  2  1  0 12 13  7]\n",
      "********************\n",
      "Cluster 0 : [('criar', 15), ('bolsonaro', 8), ('ser', 7), ('forte', 7), ('brasil', 7)]\n",
      " — — — — — — — — —\n",
      "Cluster 1 : [('infecção', 13), ('tratamento', 12), ('desenvolver', 10), ('tratar', 8), ('ério', 8)]\n",
      " — — — — — — — — —\n",
      "Cluster 2 : [('mecanismo', 12), ('ser', 11), ('gene', 10), ('parede', 9), ('celular', 9)]\n",
      " — — — — — — — — —\n",
      "Cluster 3 : [('una', 20), ('resistència', 14), ('per', 13), ('als', 9), ('antibiòtics', 7)]\n",
      " — — — — — — — — —\n",
      "Cluster 4 : [('gene', 8), ('coisa', 6), ('humano', 4), ('lagarta', 3), ('soja', 3)]\n",
      " — — — — — — — — —\n",
      "Cluster 5 : [('gene', 12), ('reduzir', 10), ('estudo', 9), ('base', 8), ('infecção', 8)]\n",
      " — — — — — — — — —\n",
      "Cluster 6 : [('anel', 5), ('outro', 5), ('de', 5), ('ocorrer', 4), ('tratar', 4)]\n",
      " — — — — — — — — —\n",
      "Cluster 7 : [('alto', 11), ('hospital', 10), ('uti', 8), ('er', 6), ('neonatal', 6)]\n",
      " — — — — — — — — —\n",
      "Cluster 8 : [('problema', 10), ('antimicrobiano', 4), ('hospital', 4), ('esperar', 4), ('ir', 4)]\n",
      " — — — — — — — — —\n",
      "Cluster 9 : [('prova', 5), ('criar', 4), ('banho', 4), ('virar', 3), ('burro', 3)]\n",
      " — — — — — — — — —\n",
      "Cluster 10 : [('criar', 31), ('tomar', 14), ('falar', 14), ('passar', 14), ('gente', 12)]\n",
      " — — — — — — — — —\n",
      "Cluster 11 : [('tomar', 170), ('criar', 167), ('ficar', 58), ('remédio', 50), ('ter', 49)]\n",
      " — — — — — — — — —\n",
      "Cluster 12 : [('de', 12), ('efeito', 12), ('medicamento', 12), ('em', 8), ('criar', 6)]\n",
      " — — — — — — — — —\n",
      "Cluster 13 : [('caso', 7), ('microrganismo', 6), ('ano', 5), ('klebsiella', 5), ('pneumoniae', 5)]\n",
      " — — — — — — — — —\n",
      "Cluster 14 : [('criar', 89), ('uso', 68), ('vírus', 61), ('ser', 57), ('medicamento', 44)]\n",
      " — — — — — — — — —\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# topics sorted by the number of documents they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):',top_index)\n",
    "print('*'*20)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4268/4268 [00:01<00:00, 3300.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of documents with topic allocated: 4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "topic_names = ['Cluster 0,',\n",
    "                'Cluster 1,',\n",
    "                'Cluster 2,',\n",
    "                'Cluster 3,',\n",
    "                'Cluster 4,',\n",
    "                'Cluster 5,',\n",
    "                'Cluster 6,',\n",
    "                'Cluster 7,',\n",
    "                'Cluster 8,',\n",
    "                'Cluster 9,']\n",
    "\n",
    "for i, topic_num in enumerate(topic_indices):\n",
    " topic_dict[topic_num]=topic_names[i]\n",
    "\n",
    "\n",
    "\n",
    "topic_allocation(tweets_df, docs, mgp, topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_excel('sttm_10topics_results.xlsx', index = False,  header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
